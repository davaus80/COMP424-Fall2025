# Student agent: Add your own agent here
from agents.agent import Agent
from store import register_agent
import sys

import numpy as np
from copy import deepcopy
from helpers import random_move, execute_move, check_endgame, get_valid_moves, MoveCoordinates, get_directions, get_two_tile_directions, count_disc_count_change

# Lightweight timing profiler for method-level benchmarking
import time
import functools


#------------- Debug tools ---------------- #

class SimpleProfiler:
  def __init__(self):
    self.data = {}  # label -> {'time': float, 'count': int}

  def _record(self, label, elapsed):
    d = self.data.setdefault(label, {'time': 0.0, 'count': 0})
    d['time'] += elapsed
    d['count'] += 1

  def profile(self, label):
    def decorator(fn):
      @functools.wraps(fn)
      def wrapper(*args, **kwargs):
        t0 = time.perf_counter()
        try:
          return fn(*args, **kwargs)
        finally:
          t1 = time.perf_counter()
          self._record(label, t1 - t0)
      return wrapper
    return decorator

  def report(self, top=10):
    items = sorted(self.data.items(), key=lambda kv: kv[1]['time'], reverse=True)[:top]
    out = ["Profiler report (label, total_time_s, calls, avg_s):"]
    for label, v in items:
      avg = v['time'] / v['count'] if v['count'] else 0.0
      out.append(f"{label:30} {v['time']:.6f}s  {v['count']:6d}  avg={avg:.6f}s")
    return "\n".join(out)


PROFILER = SimpleProfiler()


offsets = np.array([(-1, 0), (1, 0), (0, -1), (0, 1),
                    (-1, -1), (-1, 1), (1, -1), (1, 1)] + 

                   [(-2, 0), (2, 0), (0, -2), (0, 2), 
                    (-2, 1), (2, 1), (1, -2), (1, 2), 
                    (-2, -1), (2, -1), (-1, -2), (-1, 2),
                    (-2, -2), (-2, 2), (2, -2), (2, 2)], dtype = int)

def _get_valid_moves(chess_board,player:int) -> list[MoveCoordinates]:
    """
    Vectorized get_valid_moves using numpy broadcasting.
    Returns a list[MoveCoordinates].
    """
    board_h, board_w = chess_board.shape
    # locate source pieces
    src_rows, src_cols = np.nonzero(chess_board == player)

    if src_rows.size == 0:
        return []

    # all offsets to consider
    M = offsets.shape[0]

    # create (N,1,2) src array and broadcast with (1,M,2) offsets -> (N,M,2) dests
    src = np.stack((src_rows, src_cols), axis=1)[:, None, :]  # (N,1,2)
    dests = src + offsets[None, :, :]                         # (N,M,2)

    # flatten dest coordinates and corresponding src repeats
    dest_rows = dests[..., 0].ravel()
    dest_cols = dests[..., 1].ravel()
    src_rows_rep = np.repeat(src_rows, M)
    src_cols_rep = np.repeat(src_cols, M)

    # mask: dests inside board
    in_bounds = (dest_rows >= 0) & (dest_rows < board_h) & (dest_cols >= 0) & (dest_cols < board_w)
    if not np.any(in_bounds):
        return []

    dest_rows_ib = dest_rows[in_bounds].astype(int)
    dest_cols_ib = dest_cols[in_bounds].astype(int)
    src_rows_ib = src_rows_rep[in_bounds].astype(int)
    src_cols_ib = src_cols_rep[in_bounds].astype(int)

    # mask: dest empty
    empty_mask = (chess_board[dest_rows_ib, dest_cols_ib] == 0)
    if not np.any(empty_mask):
        return []

    final_src_rows = src_rows_ib[empty_mask]
    final_src_cols = src_cols_ib[empty_mask]
    final_dest_rows = dest_rows_ib[empty_mask]
    final_dest_cols = dest_cols_ib[empty_mask]

    # build MoveCoordinates list
    moves = [MoveCoordinates((int(sr), int(sc)), (int(dr), int(dc)))
             for sr, sc, dr, dc in zip(final_src_rows, final_src_cols, final_dest_rows, final_dest_cols)]

    return moves


def print_tree(node, prefix: str = "", is_tail: bool = True):
  """Print tree sideways with branches going upward."""
  if node.parent:
    UCT = node.parent.UCB1(node, c=1.4)
    dest = node.action.get_dest()
    source = node.action.get_src()
  else:
    UCT = 0.0
    dest = []
    source = []

  if node.children:
    for i, child in enumerate(node.children[:-1]):
      print_tree(child, prefix + ("│   " if not is_tail else "    "), False)
    print_tree(node.children[-1], prefix + ("│   " if not is_tail else "    "), True)
  print(prefix + ("└── " if is_tail else "┌── ") + f"[{node.minmax}] {node.wins}/{node.visits} UCT:{UCT: .3} {source}{dest}")
  


@register_agent("student_agent")
class StudentAgent(Agent):
  """
  A class for your implementation. Feel free to use this class to
  add any helper functionalities needed for your agent.
  """

  def __init__(self):
    super(StudentAgent, self).__init__()
    self.name = "StudentAgent"
    
    self.last_known_node = None
    self.number_moves = 0

  @PROFILER.profile("StudentAgent.mcts_search")
  def mcts_search(self, root_state, player, iter=800):
    
    root = None

    if self.last_known_node:

      #reuse the existing tree for the subsequent iterations
      #by matching the child of our choise with the actual opponent choice that lead to the new state
      #Possible weird edge case: having back to back turns
      for child in self.last_known_node.children:
        if np.array_equal(child.state, root_state):
          root = child
          break
    

    if root == None:
      root = MCTSNode(root_state, player, agent_player_id=player)

    for _ in range(iter):
      node = root
      
      
      while not node.is_terminal() and node.is_fully_expanded():
        # print("First best child")
        node = node.best_child()

      if not node.is_terminal():
        node = node.expand()

      # Simulation
      # result = node.limited_rollout()
      result = node.greedy_rollout()
      # result = node.super_fast_rollout()


      # Backpropagation
      node.backpropagate(result)

    print_tree(root)
    print("-"*60)

    self.last_known_node = root.best_child(c=0)

    return self.last_known_node.action

  def greedy_step(self, board, color, opponent):
        """
        Choose a move based on a simple Ataxx heuristic.

        Parameters:
        - board: 2D numpy array representing the game board.
        - color: Integer representing the agent's color (1 for Player 1/Blue, 2 for Player 2/Brown).
        - opponent: Integer representing the opponent's color.

        Returns:
        - MoveCoordinates: The chosen move.
        """
        # Get all legal moves for the current player
        legal_moves = get_valid_moves(board, color)

        if not legal_moves:
            return None  # No valid moves available, pass turn

        # Apply heuristic: maximize piece difference, corner control, and minimize opponent mobility
        best_move = None
        best_score = float('-inf')

        for move in legal_moves:
            simulated_board = np.copy(board)
            execute_move(simulated_board, move, color)
            # evaluate by piece difference, corner bonus, and opponent mobility
            move_score = self.evaluate_board(simulated_board, color, opponent)

            if move_score > best_score:
                best_score = move_score
                best_move = move

        # Return the best move found (or random fallback)
        return best_move 


  def evaluate_board(self, board, color, opponent):

        # piece difference
        player_count = np.count_nonzero(board == color)
        opp_count = np.count_nonzero(board == opponent)
        score_diff = player_count - opp_count
        # corner control bonus
        n = board.shape[0]
        corners = [(0, 0), (0, n - 1), (n - 1, 0), (n - 1, n - 1)]
        corner_bonus = sum(1 for (i, j) in corners if board[i, j] == color) * 5
        # penalize opponent mobility
        opp_moves = len(get_valid_moves(board, opponent))
        mobility_penalty = -opp_moves
        return score_diff + corner_bonus + mobility_penalty

  def step(self, chess_board, player, opponent):
    """
    Implement the step function of your agent here.
    You can use the following variables to access the chess board:
    - chess_board: a numpy array of shape (board_size, board_size)
      where 0 represents an empty spot, 1 represents Player 1's discs (Blue),
      and 2 represents Player 2's discs (Brown).
    - player: 1 if this agent is playing as Player 1 (Blue), or 2 if playing as Player 2 (Brown).
    - opponent: 1 if the opponent is Player 1 (Blue), or 2 if the opponent is Player 2 (Brown).

    You should return a tuple (r,c), where (r,c) is the position where your agent
    wants to place the next disc. Use functions in helpers to determine valid moves
    and more helpful tools.

    Please check the sample implementation in agents/random_agent.py or agents/human_agent.py for more details.
    """


    start_time = time.time()

    next_move = self.mcts_search(chess_board, player)

    # arr = chess_board
    # dest = next_move.get_dest()
    # r, c = dest
    # h, w = arr.shape

    # offsets = [(-1,0),(1,0),(0,-1),(0,1)]
    # offsets += [(-1,-1),(-1,1),(1,-1),(1,1)]

    # nbrs = [(r+dr, c+dc) for dr, dc in offsets if 0 <= r+dr < h and 0 <= c+dc < w]
    # if not nbrs:
    #     return np.zeros_like(arr, dtype=bool), []

    # rows, cols = zip(*nbrs)
    # rows = np.array(rows, dtype=int)
    # cols = np.array(cols, dtype=int)

    # vals_equal = (arr[rows, cols] == player)
    # mask = np.zeros_like(arr, dtype=bool)
    # mask[rows[vals_equal], cols[vals_equal]] = True

    # coords = [(int(rr), int(cc)) for rr, cc in zip(rows[vals_equal], cols[vals_equal])]

    # if len(coords) !=0:
    #   next_move = MoveCoordinates(coords[0], dest)

    

    time_taken = time.time() - start_time

    print("My AI's turn took ", time_taken, "seconds.")

    # Print profiler summary for this step
    # print(PROFILER.report(top=10))

    self.number_moves += 1

    return next_move

class MCTSNode:
  def __init__(self, state, player:int, agent_player_id: int | None = None, parent=None, action=None, rng=np.random.default_rng()):
    """
    state         : board state for this node
    player        : player to move at this node (1 or 2)
    agent_player_id: which player id is "our" agent (used to evaluate rollouts)
    """
    self.state = state

    # agent player id (who we consider the "maximizing" agent)
    self.agent_player_id = player if agent_player_id is None else agent_player_id

    self.parent = parent
    self.action = action
    self.children = []

    # player to move at this node
    self.player = player

    # minmax flag: 0 means node is for agent (max), 1 means opponent (min)
    self.minmax = 0 if self.player == self.agent_player_id else 1

    self.visits = 0.0
    self.wins  = 0.0

    self.rng = rng

    #used to bias search 
    self.ratio = self.board_ratio()
    self.coin_delta = self.disk_delta()

    # Initialize list of legal moves for this node's player
    self.untried_action = _get_valid_moves(self.state, self.player)
    self.number_legal_moves = len(self.untried_action)



  @PROFILER.profile("MCTSNode.is_terminal")
  def is_terminal(self) -> bool:
    return self.number_legal_moves == 0
  
  @PROFILER.profile("MCTSNode.is_fully_expanded")
  def is_fully_expanded(self):
    return len(self.untried_action) == 0  

  @PROFILER.profile("MCTSNode.expand")
  def expand(self):
    """
    Expand new child node
    """

    action = self.untried_action.pop(self.rng.integers(0,len(self.untried_action))) #randomized to eliminate risk or structure
    new_state = deepcopy(self.state)

    execute_move(new_state, action, self.player)
    new_player = 3 - self.player #flips the player
 

    child = MCTSNode(new_state, new_player, self.agent_player_id, parent=self, action=action, rng=self.rng) 
    self.children.append(child)
    return child

  @PROFILER.profile("MCTSNode.best_child")
  def best_child(self, c=0.7):
    """
    Return best child using upper confidence tree comparison.  
    """
    return max(self.children, key=lambda child: self.UCB1(child, c))


  @PROFILER.profile("MCTSNode.UCB1")
  def UCB1(self, child, c) -> float:
    # keep infinite score for unvisited children so they get explored
    if child.visits == 0:
      return float("inf")
    # standard UCB: value + c * sqrt( log(parent_visits) / child_visits )
    return (float(child.wins) / float(child.visits)) + c * np.sqrt(np.log(max(1.0, float(self.visits))) / float(child.visits))

  #---------------------- Board eval functions ------------------------#

  @PROFILER.profile("MCTSNode.ratio")
  def board_ratio(self) -> float:

    """"
    Ratio of friendly to oppoent tiles
    """

    friendly_col, _ = np.nonzero(self.state == self.agent_player_id)
    enemy_col, _ = np.nonzero(self.state == (3 - self.agent_player_id))

    friendly_tiles = friendly_col.size
    enemy_tiles = enemy_col.size
    
    if enemy_tiles == 0:
      return float("inf")

    return friendly_tiles/enemy_tiles
  

  @PROFILER.profile("MCTSNode.delta")
  def disk_delta(self) -> int:
    """
    Calculate the change in disks after the move
    """

    if self.parent:
      our_score = np.count_nonzero(self.state == self.agent_player_id)
      parent_score = np.count_nonzero(self.parent.state == self.agent_player_id)
      
      return our_score - parent_score
    else:
      return 0


  def mobility_delta(self) -> int:
    """Returns the change in number of legal moves from parent to child node"""

    if self.parent:
      our_score = self.number_legal_moves
      parent_score = self.parent.number_legal_moves
        
      return our_score - parent_score
    
    else:
      return 0

  
  #---------------------------- Deafault policies -----------------------#

  @PROFILER.profile("MCTSNode.rollout")
  def rollout(self) -> float:
    """
    Simulate game until completion. Return reward in [0,1] for agent_player_id:
      1.0 = agent win, 0.5 = draw, 0.0 = agent loss
    """
    curr_state = deepcopy(self.state)
    curr_player = self.player

    stuck_flag = False

    while True:
      allowed_moves = _get_valid_moves(curr_state, curr_player)
      number_allowed_moves = len(allowed_moves)

      if number_allowed_moves == 0:
        if stuck_flag:
          return 0.5
        curr_player = 3 - curr_player
        stuck_flag = True
        continue

      move = allowed_moves[self.rng.integers(0, number_allowed_moves)]
      execute_move(curr_state, move, curr_player)
      is_endgame, p1, p2 = check_endgame(curr_state)
      if is_endgame:
        if p1 > p2:
          winner = 1
        elif p2 > p1:
          winner = 2
        else:
          return 0.5
        # normalize reward to [0,1] from agent perspective
        return 1.0 if winner == self.agent_player_id else 0.0

      curr_player = 3 - curr_player
      stuck_flag = False

  @PROFILER.profile("MCTSNode.lim_rollout")
  def limited_rollout(self) -> float:
    depth_limit = 12

    curr_state = deepcopy(self.state)
    curr_player = self.player

    stuck_flag = False
    n = 0

    while n < depth_limit:
      allowed_moves = _get_valid_moves(curr_state, curr_player)
      number_allowed_moves = len(allowed_moves)

      if number_allowed_moves == 0:
        if stuck_flag:
          break
        
        curr_player = 3 - curr_player
        stuck_flag = True  
        continue

      move = allowed_moves[self.rng.integers(0, number_allowed_moves)]
      execute_move(curr_state, move, curr_player)

      curr_player = 3 - curr_player
      stuck_flag = False
      n += 1

    our = self.agent_player_id
    opp = 3 - self.agent_player_id

    our_coins = np.count_nonzero(curr_state == our)
    opp_coins = np.count_nonzero(curr_state == opp)

    player_count = np.count_nonzero(curr_state == our)
    opp_count = np.count_nonzero(curr_state == opp)

    score_diff = player_count - opp_count
    # corner control bonus
    n = curr_state.shape[0]
    corners = [(0, 0), (0, n - 1), (n - 1, 0), (n - 1, n - 1)]
    corner_bonus = sum(1 for (i, j) in corners if curr_state[i, j] == our)*2

    our_score = our_coins 
    opp_score = opp_coins 
    
    # return (score_diff)/10

    if our_score > opp_score:
      return 1.0
    elif our_score < opp_score:
      return 0.0
    else:
      return 0.5
    
  def quick_greed(self, board_ref, place_ref, move, player):
    place_ref = place_ref*0 + board_ref
    execute_move(place_ref, move, player)

    opp = 3- player
    p_score = np.count_nonzero(place_ref == player)
    o_score = np.count_nonzero(place_ref == opp)

    return p_score - o_score


  def greedy_rollout(self) -> float:
    depth_limit = 5

    save_state = deepcopy(self.state)
    curr_state = deepcopy(self.state)


    curr_player = self.player

    stuck_flag = False
    n = 0

    while n < depth_limit:
      allowed_moves = _get_valid_moves(curr_state, curr_player)
      number_allowed_moves = len(allowed_moves)

      if number_allowed_moves == 0:
        if stuck_flag:
          break
        
        curr_player = 3 - curr_player
        stuck_flag = True  
        continue
      
      next_move = max(allowed_moves, key= lambda x: self.quick_greed(curr_state, save_state, x, curr_player)) 
      execute_move(curr_state, next_move, curr_player)

      curr_player = 3 - curr_player
      stuck_flag = False
      n += 1

    our = self.agent_player_id
    opp = 3 - self.agent_player_id

    our_coins = np.count_nonzero(curr_state == our)
    opp_coins = np.count_nonzero(curr_state == opp)

    # player_count = np.count_nonzero(curr_state == our)
    # opp_count = np.count_nonzero(curr_state == opp)

    # score_diff = player_count - opp_count
    # # corner control bonus
    # n = curr_state.shape[0]
    # corners = [(0, 0), (0, n - 1), (n - 1, 0), (n - 1, n - 1)]
    # corner_bonus = sum(1 for (i, j) in corners if curr_state[i, j] == our)*2

    our_score = our_coins 
    opp_score = opp_coins 
    
    # return (score_diff)/10

    if our_score > opp_score:
      return 1.0
    elif our_score < opp_score:
      return 0.0
    else:
      return 0.5

  


  def super_fast_rollout(self) -> float:
    depth_limit = 5

    curr_state = deepcopy(self.state)


    curr_player = self.player

    stuck_flag = False
    n = 0

    while n < depth_limit:
      allowed_moves = _get_valid_moves(curr_state, curr_player)
      number_allowed_moves = len(allowed_moves)

      if number_allowed_moves == 0:
        if stuck_flag:
          break
        
        curr_player = 3 - curr_player
        stuck_flag = True  
        continue
      
      next_move = allowed_moves[0]
      execute_move(curr_state, next_move, curr_player)

      curr_player = 3 - curr_player
      stuck_flag = False
      n += 1

    our = self.agent_player_id
    opp = 3 - self.agent_player_id

    our_coins = np.count_nonzero(curr_state == our)
    opp_coins = np.count_nonzero(curr_state == opp)

    # player_count = np.count_nonzero(curr_state == our)
    # opp_count = np.count_nonzero(curr_state == opp)

    # score_diff = player_count - opp_count
    # # corner control bonus
    # n = curr_state.shape[0]
    # corners = [(0, 0), (0, n - 1), (n - 1, 0), (n - 1, n - 1)]
    # corner_bonus = sum(1 for (i, j) in corners if curr_state[i, j] == our)*2

    our_score = our_coins 
    opp_score = opp_coins 
    
    # return (score_diff)/10

    if our_score > opp_score:
      return 1.0
    elif our_score < opp_score:
      return 0.0
    else:
      return 0.5


  @PROFILER.profile("MCTSNode.backpropagate")
  def backpropagate(self, result) -> None:
    """
    Update tree
    """
    self.visits += 1
    self.wins += result

    if self.parent:
      self.parent.backpropagate(result)
























